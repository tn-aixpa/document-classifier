{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4439595-9c98-4fbe-85bc-9508056d2497",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, DatasetDict, Dataset\n",
    "import torch\n",
    "import digitalhub as dh\n",
    "import pandas as pd\n",
    "import requests\n",
    "import os\n",
    "from transformers import AutoConfig, TrainingArguments, EarlyStoppingCallback, Trainer\n",
    "import tqdm as notebook_tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e6507b9-b26c-493e-a07f-203d260f670e",
   "metadata": {},
   "outputs": [],
   "source": [
    "proj = dh.get_or_create_project('family-audit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27995110-1c9a-4346-9c50-71c4e5f95d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_folder = 'src'\n",
    "if not os.path.exists(new_folder):\n",
    "    os.makedirs(new_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dae6b718-bdbd-4e0c-8c74-57d3a20d47f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile \"src/train.py\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import torch\n",
    "from numpyencoder import NumpyEncoder\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from datasets import Dataset, DatasetDict\n",
    "from os import path, makedirs, listdir\n",
    "\n",
    "class Dataloader:\n",
    "    def __init__(self, file_path, file_type=None, label_column='label', test_size=0.2, val_size=0.25, random_state=25, **kwargs):\n",
    "        \"\"\"\n",
    "        Initialize the Dataloader object for training CustomBertForSequenceClassification\n",
    "        Args:\n",
    "            file_path (str): Path to the parquet file.\n",
    "            label_column (str): The column name representing labels in the dataset.\n",
    "            test_size (float): Proportion of the dataset to use as the test set.\n",
    "            val_size (float): Proportion of the train/validation split to use as the validation set.\n",
    "            random_state (int): Seed for reproducibility.\n",
    "        \"\"\"\n",
    "        self.file_path = file_path\n",
    "        self.file_type = file_path.split('.')[-1]\n",
    "        self.label_column = label_column\n",
    "        self.test_size = test_size\n",
    "        self.val_size = val_size\n",
    "        self.random_state = random_state\n",
    "        self.kwargs = kwargs\n",
    "        self.df = None\n",
    "        self.dataset = None\n",
    "        self.num_labels = None\n",
    "        self.class_weights = None\n",
    "        self.encoding, self.reverse_encoding = None, None\n",
    "\n",
    "    def load_data(self):\n",
    "            \"\"\"Loads the file and prepares the dataset.\"\"\"\n",
    "            loaders = {\n",
    "                'csv': pd.read_csv,\n",
    "                'gzip': pd.read_parquet,\n",
    "                'excel': pd.read_excel,\n",
    "                'json': pd.read_json,\n",
    "                'feather': pd.read_feather,\n",
    "            }\n",
    "    \n",
    "            if self.file_type not in loaders:\n",
    "                raise ValueError(f\"Unsupported file type: {self.file_type}\")\n",
    "            self.df = loaders[self.file_type](self.file_path, **self.kwargs).reset_index()\n",
    "    \n",
    "            # Labels should start from 0; they will be mapped back\n",
    "            # when saving the predicted results\n",
    "            if self.df[self.label_column].min() == 1:\n",
    "                self.df[self.label_column] -= 1\n",
    "    \n",
    "            # Map labels to dense for CrossEntropyL (the italian BERT doesn't like sparse arrays)\n",
    "            unique_labels, label_counts = np.unique(self.df[self.label_column], return_counts=True)\n",
    "            self.num_labels = len(unique_labels)\n",
    "            self.encoding = {label:idx for idx,label in enumerate(unique_labels)}\n",
    "            self.reverse_encoding = {idx:label for idx,label in enumerate(unique_labels)}\n",
    "            self.df[self.label_column] = self.df[self.label_column].map(self.encoding)\n",
    "            # saving the reverse indexing\n",
    "            with open(\"reverse_encoding.json\", \"w\") as f:\n",
    "                json.dump(self.reverse_encoding, f,\n",
    "                          indent=4, sort_keys=True,\n",
    "                          separators=(', ', ': '), ensure_ascii=False,\n",
    "                          cls=NumpyEncoder)\n",
    "    \n",
    "            # Class weights\n",
    "            inverse = 1 / label_counts\n",
    "            normalized_weights = inverse / inverse.sum()\n",
    "            print(self.df.head())\n",
    "            # self.class_weights = torch.FloatTensor(normalized_weights).to('cuda')\n",
    "\n",
    "\n",
    "    def stratified_split(self):\n",
    "        \"\"\"\n",
    "        Performs stratified train/validation/test split.\n",
    "        Given that the data has many labels and they are often unequally distributed,\n",
    "        many classes can be not represented at all in the training session.\n",
    "        This method forces similar class distributions.\n",
    "        If some classes are in less than 3 observations, it will raise an error.\n",
    "        \"\"\"\n",
    "        if self.df is None:\n",
    "            raise ValueError(\"Data not loaded. Call load_data() first.\")\n",
    "\n",
    "        # First split: train+val/test\n",
    "        split = StratifiedShuffleSplit(n_splits=1, test_size=self.test_size, random_state=self.random_state)\n",
    "        train_val_idx, test_idx = next(split.split(self.df, self.df[self.label_column]))\n",
    "        train_val_data = self.df.iloc[train_val_idx].reset_index(drop=True)\n",
    "        test_set = self.df.iloc[test_idx].reset_index(drop=True)\n",
    "\n",
    "        # Second split: train/validation\n",
    "        split = StratifiedShuffleSplit(n_splits=1, test_size=self.val_size, random_state=self.random_state)\n",
    "        train_idx, val_idx = next(split.split(train_val_data, train_val_data[self.label_column]))\n",
    "        train_set = train_val_data.iloc[train_idx].reset_index(drop=True)\n",
    "        val_set = train_val_data.iloc[val_idx].reset_index(drop=True)\n",
    "\n",
    "        # Convert to Hugging Face DatasetDict\n",
    "        self.dataset = DatasetDict({\n",
    "            'train': Dataset.from_pandas(train_set),\n",
    "            'validation': Dataset.from_pandas(val_set),\n",
    "            'test': Dataset.from_pandas(test_set)\n",
    "        })\n",
    "\n",
    "    def get_dataset(self):\n",
    "        \"\"\"\n",
    "        Retrieves the dataset dictionary containing train, validation, and test sets.\n",
    "\n",
    "        Returns:\n",
    "            DatasetDict: A dictionary containing the splits as Hugging Face datasets.\n",
    "        \"\"\"\n",
    "        if self.dataset is None:\n",
    "            raise ValueError(\"Dataset not created. Call stratified_split() first.\")\n",
    "        return self.dataset\n",
    "\n",
    "    def get_class_weights(self):\n",
    "        \"\"\"\n",
    "        Retrieves the class weights.\n",
    "\n",
    "        Returns:\n",
    "            torch.FloatTensor: Class weights for handling imbalanced classes.\n",
    "        \"\"\"\n",
    "        if self.class_weights is None:\n",
    "            raise ValueError(\"Class weights not computed. Call load_data() first.\")\n",
    "        return self.class_weights\n",
    "\n",
    "    def get_num_labels(self):\n",
    "        \"\"\"\n",
    "        Retrieves the number of unique labels.\n",
    "\n",
    "        Returns:\n",
    "            int: Number of unique labels in the dataset.\n",
    "        \"\"\"\n",
    "        if self.num_labels is None:\n",
    "            raise ValueError(\"Number of labels not available. Call load_data() first.\")\n",
    "        return self.num_labels\n",
    "\n",
    "    def get_encoding(self):\n",
    "        \"\"\"\n",
    "        Retrieves the mapping of labels to categories.\n",
    "\n",
    "        Returns:\n",
    "            dict: Mapping of label IDs to their respective categories.\n",
    "        \"\"\"\n",
    "        if self.encoding is None:\n",
    "            raise ValueError(\"Label mapping nonexistent. Call load_data() first.\")\n",
    "        return self.encoding\n",
    "\n",
    "    def get_r_encoding(self):\n",
    "        \"\"\"\n",
    "        Retrieves the mapping of labels to categories.\n",
    "\n",
    "        Returns:\n",
    "            dict: Mapping of label IDs to their respective categories.\n",
    "        \"\"\"\n",
    "        if self.reverse_encoding is None:\n",
    "            raise ValueError(\"Label mapping nonexistent. Call load_data() first.\")\n",
    "        return self.reverse_encoding\n",
    "\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "class TokenizerFunction:\n",
    "    def __init__(self, model_name, max_length=512, use_fast=True, use_cache=False):\n",
    "        \"\"\"\n",
    "        Initialize the tokenizer function wrapper ðŸŒ¯\n",
    "        Args:\n",
    "            model_name (str): Name of the pre-trained model.\n",
    "            max_length (int): Maximum sequence length for tokenization.\n",
    "            use_fast (bool): Whether to use the fast tokenizer implementation.\n",
    "            use_cache (bool): Whether to cache the tokenizer results.\n",
    "        \"\"\"\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name,\n",
    "                                                        use_fast=use_fast,\n",
    "                                                        use_cache=use_cache)\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __call__(self, data):\n",
    "        \"\"\"\n",
    "        Tokenizes the input data.\n",
    "\n",
    "        Args:\n",
    "            data (dict): A dictionary containing the 'text' field to tokenize.\n",
    "\n",
    "        Returns:\n",
    "            dict: Tokenized data including input IDs, attention masks, etc.\n",
    "        \"\"\"\n",
    "        return self.tokenizer(data['text'],\n",
    "                              padding='max_length',\n",
    "                              truncation=True,\n",
    "                              max_length=self.max_length,\n",
    "                              return_tensors=\"pt\")\n",
    "\n",
    "    def get_tokenizer(self):\n",
    "        return self.tokenizer\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from torchmetrics.classification import MulticlassF1Score, Accuracy\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification, AutoModel\n",
    "from transformers import PreTrainedModel\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "from transformers import AutoConfig, TrainingArguments, EarlyStoppingCallback, Trainer\n",
    "\n",
    "from datetime import date, datetime\n",
    "\n",
    "# import pandas as pd\n",
    "\n",
    "import json\n",
    "# import wandb\n",
    "\n",
    "class BertForSentenceClassification(PreTrainedModel):\n",
    "\n",
    "    \"\"\"\n",
    "    BERT architecture is intended to be from \"dbmdz/bert-base-italian-xxl-cased\"\n",
    "    but other models can be tried.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    def __init__(self, config, model_name, num_labels, class_weights=None):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = num_labels\n",
    "        self.bert = AutoModel.from_pretrained(model_name)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)\n",
    "        self.class_weights = class_weights\n",
    "        self.accuracy = Accuracy(num_classes=num_labels, task='multiclass')\n",
    "        self.f1 = MulticlassF1Score(num_classes=num_labels, average='micro') # changed weight\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, labels=None):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = self.classifier(outputs.last_hidden_state[:, 0, :])\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "\n",
    "            loss_fct = nn.CrossEntropyLoss(label_smoothing=0.1) #,weight=self.class_weights\n",
    "            loss = loss_fct(logits, labels)\n",
    "\n",
    "            f1_score = self.f1(logits.argmax(dim=1), labels)\n",
    "            accuracy_score = self.accuracy(logits.argmax(dim=1), labels)\n",
    "            # wandb.log({\n",
    "            #     \"f1_score\": f1_score,\n",
    "            #     \"accuracy\": accuracy_score,\n",
    "            #     'CrossEntropyLoss': loss.item() if loss is not None else None\n",
    "            # })\n",
    "            # print(\"F1 score\", f1_score)\n",
    "            # print(\"accuracy\", accuracy_score)\n",
    "            # print(\"CrossEntropyLoss\", loss.item())\n",
    "\n",
    "        return SequenceClassifierOutput(loss=loss, logits=logits)\n",
    "\n",
    "class TrainerHandler:\n",
    "    def __init__(self, trainer, tokenized_datasets, num_labels, encoding, model_name,\n",
    "                 model_save_path):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            trainer: Hugging Face Trainer instance.\n",
    "            tokenized_datasets: DatasetDict with train/val/test splits.\n",
    "            num_labels: Number of labels in the classification task.\n",
    "            encoding: Mapping from dense to sparse original labelling.\n",
    "            model_name\n",
    "            model_save_path: Path to save the configuration, weights and tokenizer for reproducibility.\n",
    "        \"\"\"\n",
    "        self.trainer = trainer\n",
    "        self.tokenized_datasets = tokenized_datasets\n",
    "        self.num_labels = num_labels\n",
    "        self.encoding = encoding\n",
    "        self.model_name = model_name\n",
    "        self.model_save_path = model_save_path\n",
    "\n",
    "    def compute_f1_score(self, predictions):\n",
    "        \"\"\"\n",
    "        Micro F1 score from\n",
    "\n",
    "        Args:\n",
    "            predictions: Output from the trainer.predict() method.\n",
    "\n",
    "        Returns:\n",
    "            f1_score: The calculated F1 score.\n",
    "        \"\"\"\n",
    "        logits = torch.tensor(predictions.predictions)\n",
    "        labels = torch.tensor(predictions.label_ids)\n",
    "        f1 = MulticlassF1Score(num_classes=self.num_labels, average='micro') # weight\n",
    "        f1_score = f1(logits.argmax(dim=1), labels.float())\n",
    "        return f1_score, logits, labels\n",
    "\n",
    "    def save_f1_results(self, f1_score):\n",
    "        \"\"\"\n",
    "        Save the F1 score to a CSV file.\n",
    "\n",
    "        Args:\n",
    "            f1_score: by default, it's the micro F1 weighted on class frequency.\n",
    "        \"\"\"\n",
    "        now = datetime.today()\n",
    "        pd.DataFrame({\n",
    "            'F1': [f1_score.item()],\n",
    "            'modello': [self.model_name],\n",
    "            'T': [now],\n",
    "        }).to_csv(f'{self.model_name}.csv')\n",
    "        print(f\"F1 score saved to {self.model_name}.csv\")\n",
    "\n",
    "    def save_predictions(self, logits, labels):\n",
    "        \"\"\"\n",
    "        Save predicted and true labels to a CSV file.\n",
    "\n",
    "        Args:\n",
    "            logits: Model logits from the predictions.\n",
    "            labels: Ground truth label indexes from the predictions.\n",
    "        \"\"\"\n",
    "        now = datetime.today()\n",
    "        inverted_encoding = {int(v): k for k, v in self.encoding.items()}\n",
    "        predicted_indices = logits.argmax(dim=1)\n",
    "        predicted_labels = [inverted_encoding[idx.item()] for idx in predicted_indices]\n",
    "        true_labels = [inverted_encoding[idx.item()] for idx in labels]\n",
    "\n",
    "        original_index = self.tokenized_datasets['test']['index'] #original indexes for test observations in the input df\n",
    "\n",
    "        results = pd.DataFrame({\n",
    "            'original_index': original_index,\n",
    "            'true_label': true_labels,\n",
    "            'predicted_label': predicted_labels,\n",
    "        })\n",
    "\n",
    "        results.true_label, results.predicted_label = results.true_label + 1, results.predicted_label + 1\n",
    "\n",
    "        file_name = f'{self.model_name}_{now.month}_{now.day}-{now.hour}_{now.minute}.csv'\n",
    "        results.to_csv(file_name, index=False)\n",
    "        print(f\"Predictions saved to {file_name}\")\n",
    "\n",
    "    def save_model_and_tokenizer(self):\n",
    "        \"\"\"\n",
    "        Save the trained model and tokenizer to the specified path.\n",
    "        \"\"\"\n",
    "        print(f\"Saving the trained model as {self.model_name}...\")\n",
    "        self.trainer.model.save_pretrained(self.model_save_path)\n",
    "        self.trainer.tokenizer.save_pretrained(self.model_save_path)\n",
    "        print(f\"Model and tokenizer saved to {self.model_save_path}\")\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"\n",
    "        Execute the training and save the outputs.\n",
    "        \"\"\"\n",
    "        print(\"Starting training...\")\n",
    "        #self.trainer.train(resume_from_checkpoint=True)\n",
    "        self.trainer.train()\n",
    "        self.trainer.save_state()\n",
    "\n",
    "        print(\"Evaluating test set...\")\n",
    "        predictions = self.trainer.predict(self.tokenized_datasets['test'])\n",
    "\n",
    "        f1_score, logits, labels = self.compute_f1_score(predictions)\n",
    "        print(f\"F1 score: {f1_score.item()}\\n\")\n",
    "\n",
    "        self.save_f1_results(f1_score)\n",
    "        self.save_predictions(logits, labels)\n",
    "        self.save_model_and_tokenizer()\n",
    "        print(\"Done.\")\n",
    "\n",
    "\n",
    "file_basepath = \"faudi_data\"\n",
    "\n",
    "def train(project, train_data, data_path = \"data/\", model_save_path = \"models/\", target_model_name = \"\"):\n",
    "\n",
    "    model_dir = f\"{file_basepath}/{model_save_path}\"\n",
    "    data_dir = f\"{file_basepath}/{data_path}\"\n",
    "    \n",
    "    try:\n",
    "        shutil.rmtree(data_dir)\n",
    "    except:\n",
    "        print(\"Error deleting data dir\")\n",
    "                \n",
    "    # Create the directory for the data\n",
    "    if not path.exists(data_dir):\n",
    "        makedirs(data_dir)\n",
    "\n",
    "    try:\n",
    "        train_data.download(data_dir) # this must change in the function\n",
    "    except:\n",
    "        print(\"Error downloading data\")\n",
    "        \n",
    "    try:\n",
    "        shutil.rmtree(model_dir)\n",
    "    except:\n",
    "        print(\"Error deleting model dir\")\n",
    "        \n",
    "     # Create the directory for the model\n",
    "    if not path.exists(model_dir):\n",
    "        makedirs(model_dir)    \n",
    "\n",
    "    model_name = \"dbmdz/bert-base-italian-xxl-cased\"\n",
    "    dataloader = Dataloader(file_path= f'{data_dir}/addestramento.gzip')\n",
    "    dataloader.load_data()\n",
    "    dataloader.stratified_split()\n",
    "    dataset = dataloader.get_dataset()\n",
    "    # class_weights = dataloader.get_class_weights()\n",
    "    num_labels = dataloader.get_num_labels()\n",
    "    encoding = dataloader.get_encoding()\n",
    "    tokenize_function = TokenizerFunction(model_name=model_name, max_length=512)\n",
    "    tokenized_datasets = (dataset.map(tokenize_function, batched=True)\n",
    "                          .shuffle(seed=25)\n",
    "                          .remove_columns(['text', 'token_type_ids']))\n",
    "    print(f'--Loading the model for predicting {num_labels} labels--')\n",
    "    config = AutoConfig.from_pretrained(model_name, num_labels=num_labels)  #for later saving the config\n",
    "    model = (BertForSentenceClassification\n",
    "         .from_pretrained(pretrained_model_name_or_path=model_name,\n",
    "                          model_name=model_name,\n",
    "                          config=config,\n",
    "                          num_labels=num_labels,\n",
    "                          )) #class_weights=class_weights\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir='tuned_model',\n",
    "        eval_strategy='epoch',\n",
    "        save_strategy='epoch',\n",
    "        save_total_limit=5,\n",
    "        num_train_epochs=1,\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=16,\n",
    "        gradient_accumulation_steps=2,\n",
    "        weight_decay=0.005,\n",
    "        learning_rate=1e-5,\n",
    "        lr_scheduler_type='linear',\n",
    "        load_best_model_at_end=True,\n",
    "    )\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_datasets['train'],\n",
    "        eval_dataset=tokenized_datasets['validation'],\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=1)]\n",
    "    )\n",
    "    trainer.tokenizer = tokenize_function.get_tokenizer()\n",
    "    \n",
    "    handler = TrainerHandler(\n",
    "        trainer=trainer,\n",
    "        tokenized_datasets=tokenized_datasets,\n",
    "        num_labels=num_labels,\n",
    "        encoding=encoding,\n",
    "        model_name=target_model_name,\n",
    "        model_save_path=model_dir\n",
    "    )\n",
    "    handler.run()\n",
    "    project.log_model(\n",
    "        name=target_model_name,\n",
    "        kind=\"huggingface\",\n",
    "        base_model=\"dbmdz/bert-base-italian-xxl-cased\",\n",
    "        # metrics=metrics,\n",
    "        source=model_dir,\n",
    "    )             \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8c9eb1b-4c88-4207-9bc3-ac00a7a324ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "func = proj.new_function(\n",
    "    name=\"train\",\n",
    "    kind=\"python\",\n",
    "    python_version=\"PYTHON3_10\",\n",
    "    code_src=\"src/train.py\",\n",
    "    handler=\"train\",\n",
    "    requirements=[\"accelerate==1.1.1\", \"datasets==3.1.0\", \"torch==2.5.1\", \"torch_tensorrt==2.5.0\", \"torchmetrics==1.6.0\", \"torchtext==0.18.0\", \"transformer_engine==1.12.0\", \"transformer_engine_cu12==1.12.0\", \"transformers==4.46.3\", \"pandas==2.2.3\", \"numpy==2.1.3\", \"numpyencoder==0.3.0\", \"scikit-learn==1.5.2\", \"scipy==1.14.1\", \"GitPython==3.1.43\", \"attrs==24.2.0\", \"async-timeout==5.0.1\", \"aiosignal==1.3.1\", \"aiohappyeyeballs==2.4.4\", \"aiohttp==3.11.9\", \"Unidecode==1.3.8\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec41d81c-c7fb-4d69-922c-3ba009f1012e",
   "metadata": {},
   "outputs": [],
   "source": [
    "artifact = proj.get_artifact(\"train_data_it\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d0f267c-7466-487a-a128-6cad9522df82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'store://family-audit/artifact/artifact/train_data_it:bz4ywox1hlf4f0b4m4gt2s29'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "artifact.key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "25e582c3-83cb-4983-9d67-c368904fd4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_run = func.run(action=\"job\", inputs={\"train_data\": artifact.key}, parameters={\"data_path\": \"/data\", \"model_save_path\": \"/model\", \"target_model_name\": \"family_audit_model\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "88dc6e35-d9b6-4878-8c2c-bfe0c7efda08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from os import path, makedirs, listdir\n",
    "\n",
    "# file_basepath = \"assets\"\n",
    "\n",
    "# def train(project, train_data, data_path = \"data/\", model_save_path = \"models/\"):\n",
    "\n",
    "#     model_dir = f\"{file_basepath}/{model_save_path}\"\n",
    "#     data_dir = f\"{file_basepath}/{data_path}\"\n",
    "    \n",
    "#     try:\n",
    "#         shutil.rmtree(data_dir)\n",
    "#     except:\n",
    "#         print(\"Error deleting data dir\")\n",
    "                \n",
    "#     # Create the directory for the data\n",
    "#     if not path.exists(data_dir):\n",
    "#         makedirs(data_dir)\n",
    "\n",
    "#     print (data_dir)\n",
    "#     try:\n",
    "#         project.get_artifact(train_data).download(data_dir)# this must change in the function\n",
    "#     except:\n",
    "#         print(\"Error downloading data\")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "7a6e5f97-9cd9-4876-bece-caf2bc6038a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train(proj, artifact.key, data_path=\"dataT1\", model_save_path=\"modelT1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139e757a-ea37-40c2-a3c0-af5ea564bf2c",
   "metadata": {},
   "source": [
    "## Original Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "ec9ef4d0-16a9-4b48-bee7-2e1721d10f83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'kind': 'huggingface', 'metadata': {'project': 'family-audit', 'name': 'already_trained', 'version': '66dbaaefea834696acdfd0c73237bbbb', 'created': '2024-12-05T14:23:25.287Z', 'updated': '2024-12-05T14:23:27.959Z', 'created_by': 'khurshid@fbk.eu', 'updated_by': 'khurshid@fbk.eu', 'embedded': False}, 'spec': {'path': 's3://datalake/family-audit/model/already_trained/66dbaaefea834696acdfd0c73237bbbb/', 'base_model': 'dbmdz/bert-base-italian-xxl-cased', 'parameters': {}, 'metrics': {}}, 'status': {'state': 'CREATED', 'files': [{'path': 'model.safetensors', 'name': 'model.safetensors', 'content_type': 'binary/octet-stream', 'size': 314252951, 'hash': 'LiteralETag:afb8427ed1871979d9fa0d99d33de780-38', 'last_modified': '2024-12-05T14:23:27+00:00'}, {'path': 'config.json', 'name': 'config.json', 'content_type': 'application/json', 'size': 3011, 'hash': 'md5:44d730eaf5cdca3f36e24a78c342236f', 'last_modified': '2024-12-05T14:23:27+00:00'}, {'path': 'label_mapping.json', 'name': 'label_mapping.json', 'content_type': 'application/json', 'size': 1569, 'hash': 'md5:f46fbc9a442f23d23462efe7f3b0c268', 'last_modified': '2024-12-05T14:23:27+00:00'}]}, 'user': 'khurshid@fbk.eu', 'project': 'family-audit', 'name': 'already_trained', 'id': '66dbaaefea834696acdfd0c73237bbbb', 'key': 'store://family-audit/model/huggingface/already_trained:66dbaaefea834696acdfd0c73237bbbb'}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proj.log_model(\n",
    "            name=\"already_trained\",\n",
    "            kind=\"huggingface\",\n",
    "            base_model=\"dbmdz/bert-base-italian-xxl-cased\",\n",
    "            # metrics=metrics,\n",
    "            source=\"./tuned_model\",\n",
    "        ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc0787ff-89a8-47b3-8379-b1c94b048a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = proj.get_model('family_audit_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a8f40c3c-413b-405e-a97d-63360fd86f5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'model'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c56f59-7bc5-4709-861c-e7e5777dfa06",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
